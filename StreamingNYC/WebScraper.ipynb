{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Apache Spark Streaming cannot connect directly to a URL, so this is a web scraper in base Python in lieu of Kafka or other streaming tools.\n",
    "\n",
    "This reads data from the [NYC Traffic Data Stream](http://207.251.86.229/nyc-links-cams/LinkSpeedQuery.txt) and writes it to a text file in intervals.\n",
    "\n",
    "This is intended to run in parallel with the PySpark script, which where the StreamingContext will read from this text file for the map visualization and traffic alerts.\n",
    "\n",
    "Since PySpark's textFileStream operator will only grab new files, we will add a time stamp to the file name, which will generate several files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "# The URL for the streaming data to be scraped\n",
    "streamPath = 'http://207.251.86.229/nyc-links-cams/LinkSpeedQuery.txt'\n",
    "\n",
    "# The output file for the script\n",
    "# Does not need to be created first\n",
    "outputFilePath = 'C:/Users/JeffM/Documents/Projects/Spark Streaming/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is used to create our timer for re-running the scraping script\n",
    "\n",
    "Credits to MestreLion [for the script](http://stackoverflow.com/questions/474528/what-is-the-best-way-to-repeatedly-execute-a-function-every-x-seconds-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from threading import Timer\n",
    "\n",
    "class RepeatedTimer(object):\n",
    "    def __init__(self, interval, function, *args, **kwargs):\n",
    "        self._timer     = None\n",
    "        self.interval   = interval\n",
    "        self.function   = function\n",
    "        self.args       = args\n",
    "        self.kwargs     = kwargs\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "\n",
    "    def _run(self):\n",
    "        self.is_running = False\n",
    "        self.start()\n",
    "        self.function(*self.args, **self.kwargs)\n",
    "\n",
    "    def start(self):\n",
    "        if not self.is_running:\n",
    "            self._timer = Timer(self.interval, self._run)\n",
    "            self._timer.start()\n",
    "            self.is_running = True\n",
    "\n",
    "    def stop(self):\n",
    "        self._timer.cancel()\n",
    "        self.is_running = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def scrapeWeb():\n",
    "    \"\"\"\n",
    "    This function will read data from the streaming path specified in the beginning.\n",
    "    It will then write it to an array, and then print it to a text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scrape the website\n",
    "    r = requests.get(streamPath, stream = False)  # Stream = false to generate a new array each time\n",
    "    \n",
    "    # Handles missing encodings\n",
    "    if r.encoding is None:\n",
    "        r.encoding = 'utf-8'\n",
    "    \n",
    "    # Array to be written to a file\n",
    "    nycTraffic = []\n",
    "    \n",
    "    # Loads data from requests to array\n",
    "    for line in r.iter_lines(decode_unicode = True):\n",
    "        # Replacing , with ; for multiple coordinates in a single column\n",
    "        updatedLine = line.replace('\"', '').replace(',', ';')\n",
    "        if len(updatedLine.split('\\t')) == 13:  # Filters out rows with missing elements\n",
    "            nycTraffic.append(updatedLine)\n",
    "    \n",
    "    # Filters out items missing rows\n",
    "    nycTraffic = [x for x in nycTraffic if len(x) > 200]\n",
    "\n",
    "    # Writes array to the text file\n",
    "    outputFile = outputFilePath+'nycTraffic_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.txt'\n",
    "    with open(outputFile, \"w\") as output:\n",
    "        writer = csv.writer(output, lineterminator = '\\n')\n",
    "        writer.writerows([line.split('\\t') for line in nycTraffic[1:]])\n",
    "    \n",
    "    # Time stamp of when the web was scraped\n",
    "    print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    \n",
    "\n",
    "print(\"Starting...\")\n",
    "# Set RepeatedTimer(n) for number of seconds between each run\n",
    "# The first job will start on a delay equal to n\n",
    "rt = RepeatedTimer(20, scrapeWeb) # Auto-starts, no need for rt.start()\n",
    "\n",
    "# Ending\n",
    "try:\n",
    "    # Set sleep(n) for number of seconds for the job to run\n",
    "    sleep(3600)  # One hour\n",
    "finally:\n",
    "    rt.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
